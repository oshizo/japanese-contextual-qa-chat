{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16607307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-27T10:13:16.490502Z",
     "start_time": "2023-08-27T10:13:16.486503Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"cyberagent/calm2-7b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed966ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-27T10:13:16.630424Z",
     "start_time": "2023-08-27T10:13:16.542705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d79c48a679a4dfebdf09c4306fb798e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_flash_attention_2=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8dd03",
   "metadata": {},
   "source": [
    "# データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5feec067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JaQuAD-version-0.1.0</td>\n",
       "      <td>{'title': '手塚治虫', 'paragraphs': [{'context': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                version                                               data\n",
       "0  JaQuAD-version-0.1.0  {'title': '手塚治虫', 'paragraphs': [{'context': '..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_tmpl = \"https://raw.githubusercontent.com/SkelterLabsInc/JaQuAD/main/data/train/jaquad_train_{i:04d}.json\"\n",
    "jaquad = pd.concat([pd.read_json(url_tmpl.format(i=i)) for i in range(30)], ignore_index=True)\n",
    "jaquad.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaquad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31748"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_qa = []\n",
    "for idx, row in jaquad.iterrows():\n",
    "    title = row[\"data\"][\"title\"]\n",
    "    for para in row[\"data\"][\"paragraphs\"]:\n",
    "        context = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            q = qa[\"question\"]\n",
    "            a = qa[\"answers\"][0][\"text\"]\n",
    "            assert len(qa[\"answers\"]) == 1\n",
    "            all_qa.append([q, a])\n",
    "len(all_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot テンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMPL = \"\"\"\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: 日本では改名には何の許可が必要ですか？\n",
    "回答: 家庭裁判所\n",
    "ASSISTANT: 日本では、改名するには家庭裁判所の許可が必要です。<|endoftext|>\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: モーリシャスの信仰で最も多いのは\n",
    "回答: ヒンドゥー教\n",
    "ASSISTANT: モーリシャスで最も信仰されている宗教はヒンドゥー教です。<|endoftext|>\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: 核反応を利用した電池を何と呼ぶ\n",
    "回答: 「原子力電池」\n",
    "ASSISTANT: 核反応を利用した電池は「原子力電池」と呼ばれます。<|endoftext|>\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: カール・マルクスの妻イェニーが、亡くなったのはいつか。\n",
    "回答: 1881年12月2日\n",
    "ASSISTANT: イェニーは1881年12月2日に亡くなりました。<|endoftext|>\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: 北京市、上海市、深圳市と共に何に分類されている？\n",
    "回答: 一線都市\n",
    "ASSISTANT: 北京市は、一線都市に分類されています。<|endoftext|>\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: 武田薬品工業の取締役は何名か\n",
    "回答: 13名\n",
    "ASSISTANT: 取締役は13名で構成されています。<|endoftext|>\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: 辺野古移設のための埋め立て工事に賛成何%？\n",
    "回答: 18.99%\n",
    "ASSISTANT: 18.99%の人が埋め立て工事に賛成しています。<|endoftext|>\n",
    "\n",
    "USER: 質問への回答をチャット形式に変換せよ\n",
    "質問: {q}\n",
    "回答: {a}\n",
    "ASSISTANT: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2e404",
   "metadata": {},
   "source": [
    "# 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改行を含むトークンをbad_wordsに指定する\n",
    "newline_ids = [[i] for i in range(65000) if \"\\n\" in tokenizer.decode(i)]\n",
    "len(newline_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68180389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T14:01:37.708015Z",
     "start_time": "2023-07-30T14:01:33.450538Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate(prompt, temperature=0.8):\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(\n",
    "            prompt, return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).to(\"cuda\")\n",
    "        gen_tokens = model.generate(\n",
    "            max_new_tokens=128,\n",
    "            input_ids=input_ids,\n",
    "            do_sample=True,\n",
    "            top_p=0.75,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=1,\n",
    "            bad_words_ids=newline_ids,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    for gen_text in tokenizer.batch_decode(\n",
    "        [tokens[len(input_ids[0]) :] for tokens in gen_tokens],\n",
    "        skip_special_tokens=False,\n",
    "    ):\n",
    "        break\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "retrycnt = 0\n",
    "\n",
    "for i, row in tqdm(enumerate(all_qa)):\n",
    "    with open(\"./result.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        q = row[0]\n",
    "        a = row[1]\n",
    "        prompt = TMPL.format(q=row[0], a=row[1])\n",
    "        while True:\n",
    "            out = generate(prompt, temperature=0.8+retrycnt*0.1).replace(tokenizer.eos_token, \"\")\n",
    "\n",
    "            # 以下の場合、temperatureを0.1上げてリトライする\n",
    "            # 1. 改行を含む\n",
    "            # 2. USERを含む（次のサンプルを生成しようとしている場合）\n",
    "            # 3. 150文字以上\n",
    "            # 4. 長さが元の質問以下\n",
    "            if \"\\n\" in out or \"USER\" in out or len(out) > 150 or len(out) <= len(a):\n",
    "                retrycnt += 1\n",
    "                if retrycnt > 25:\n",
    "                    # どうしても出来ないときは諦める\n",
    "                    break\n",
    "            else:\n",
    "                retrycnt = 0\n",
    "                break\n",
    "        f.write(json.dumps({\"question\": q, \"answer\": a, \"answer_chat\": out}, ensure_ascii=False))\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
